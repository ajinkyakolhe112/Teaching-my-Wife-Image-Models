{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a008dc",
   "metadata": {},
   "source": [
    "# Lecture 1: Neural Network Fundamentals + MNIST\n",
    "## Conceptual Understanding Approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b568036",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The Journey Begins\n",
    "### From Human Vision to Machine Learning\n",
    "\n",
    "> \"How do we teach machines to recognize what we see instantly?\"\n",
    "\n",
    "Today's Goal: Build a system that can recognize handwritten digits like a human child learning to read numbers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0962d3f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The Brain Analogy\n",
    "\n",
    "### How Does Your Brain Recognize a Number?\n",
    "ðŸ§  **Your brain**: Millions of neurons working together  \n",
    "ðŸ”— **Connections**: Neurons send signals to each other  \n",
    "âš¡ **Learning**: Connections strengthen with experience  \n",
    "ðŸŽ¯ **Recognition**: Patterns emerge from experience  \n",
    "\n",
    "### Can We Simulate This?\n",
    "Yes! That's exactly what neural networks do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05e73a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## What is a Neural Network?\n",
    "\n",
    "### Simple Analogy: A Decision Committee\n",
    "- Each \"neuron\" is like a committee member\n",
    "- They receive information (inputs)\n",
    "- They make a decision (output)\n",
    "- They vote on the final answer\n",
    "\n",
    "### The Magic: Learning from Examples\n",
    "Just like how you learned to recognize numbers by seeing many examples!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ec54f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Meet MNIST: The \"Hello World\" of Computer Vision\n",
    "\n",
    "### Why Handwritten Digits?\n",
    "âœ… **Simple**: Only 10 classes (0-9)  \n",
    "âœ… **Visual**: We can see what's happening  \n",
    "âœ… **Practical**: Real-world application  \n",
    "âœ… **Achievable**: We can get great results  \n",
    "\n",
    "### The Challenge\n",
    "28Ã—28 pixels = 784 numbers â†’ \"What digit is this?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df33b1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Building Intuition: The Learning Process\n",
    "\n",
    "### How Would You Teach a Child?\n",
    "1. **Show examples**: \"This is a 3, this is a 7...\"\n",
    "2. **Let them try**: \"What number do you think this is?\"\n",
    "3. **Correct mistakes**: \"Actually, that's a 6, not a 5\"\n",
    "4. **Practice more**: Repeat with many examples\n",
    "5. **Test understanding**: New examples they haven't seen\n",
    "\n",
    "**This is exactly what we do with neural networks!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b82cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The Architecture: 784 â†’ 128 â†’ 64 â†’ 10\n",
    "\n",
    "### Think of it as Layers of Understanding\n",
    "\n",
    "**Layer 1 (784 inputs)**: Raw pixel values  \n",
    "*\"I see some dark and light spots\"*\n",
    "\n",
    "**Layer 2 (128 neurons)**: Basic patterns  \n",
    "*\"I see some curves and lines\"*\n",
    "\n",
    "**Layer 3 (64 neurons)**: Complex features  \n",
    "*\"I see number-like shapes\"*\n",
    "\n",
    "**Layer 4 (10 outputs)**: Final decision  \n",
    "*\"This looks most like a 3!\"*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3887b17",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The Learning Magic: How Do Computers Learn?\n",
    "\n",
    "### Trial and Error, But Smarter!\n",
    "1. **Make a guess**: \"I think this is a 7\"\n",
    "2. **Check if right**: \"Oops, it was actually a 2\"  \n",
    "3. **Adjust thinking**: \"Next time I see this pattern, think 2\"\n",
    "4. **Repeat**: Do this millions of times, getting better each time\n",
    "\n",
    "### The Beautiful Part\n",
    "The computer figures out what patterns matter **on its own**!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lecture 1: Neural Network Fundamentals + MNIST Digits\n",
    "A simple, refactored implementation using torchvision datasets and Accelerate.\n",
    "\"\"\"\n",
    "#%%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "from accelerate import Accelerator\n",
    "\n",
    "def load_mnist_data():\n",
    "    \"\"\"Load and preprocess MNIST dataset\"\"\"\n",
    "    # Download the Image Dataset\n",
    "    # And Convert image to numerical value tensors\n",
    "    transform  = transforms.ToTensor()\n",
    "    train_data = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "    test_data  = datasets.MNIST('data', train=False, transform=transform)\n",
    "\n",
    "    # Create DataLoader function for efficient data-reading\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    test_loader  = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "#%%\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Network architecture: 784 -> 128 -> 64 -> 10\n",
    "        self.flatten_image = nn.Flatten()               # Convert 28x28 image to 784 pixels\n",
    "        self.hidden_layer_1 = nn.Linear(in_features = 28*28, out_features = 128)       # First hidden layer\n",
    "        self.relu_1 = nn.ReLU()                         # Activation function\n",
    "        self.hidden_layer_2 = nn.Linear(in_features = 128, out_features = 64)        # Second hidden layer\n",
    "        self.relu_2 = nn.ReLU()                         # Activation function\n",
    "        self.output_layer = nn.Linear(in_features = 64, out_features = 10)           # Output layer (10 digits)\n",
    "\n",
    "        print(\"Created neural network: 784 -> 128 -> 64 -> 10\")\n",
    "\n",
    "    def forward(self, single_batch):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        x = single_batch\n",
    "        x = self.flatten_image(x)                # Flatten image\n",
    "        x = self.relu_1(self.hidden_layer_1(x))  # First layer + activation\n",
    "        x = self.relu_2(self.hidden_layer_2(x))  # Second layer + activation\n",
    "        x = self.output_layer(x)                 # Final output (no activation)\n",
    "        return x\n",
    "\n",
    "#%%\n",
    "def train_model(model, train_loader, optimizer, accelerator, epochs=3):\n",
    "    \"\"\"Training loop with accelerate\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(x_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            accelerator.backward(loss)  # Use accelerator for backward pass\n",
    "            optimizer.step()       # Update weights\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accelerator.print(f'Epoch {epoch+1}: Average Loss = {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    \"\"\"Test the recently trained model accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Set Automatic gradient calculation OFF\n",
    "    torch.set_grad_enabled(False)\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        predictions = model(x_batch)\n",
    "        accuracy = torchmetrics.functional.accuracy(predictions, y_batch, task='multiclass', num_classes=10)\n",
    "        correct += accuracy * len(y_batch)\n",
    "        total += len(y_batch)\n",
    "    # Set Automatic gradient calculation Back On\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    print(f'Test Accuracy: {(correct/total)*100:.2f}%')\n",
    "\n",
    "#%%\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Accelerator\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # Step 1: Load data\n",
    "    train_loader, test_loader = load_mnist_data()\n",
    "\n",
    "    # Step 2: Create model and optimizer\n",
    "    model = SimpleNeuralNet()\n",
    "    PARAMETERS_TO_OPTIMIZE = model.parameters()\n",
    "    optimizer = torch.optim.SGD(params = PARAMETERS_TO_OPTIMIZE, lr=0.01)\n",
    "    \n",
    "    # Prepare everything with Accelerate\n",
    "    model, optimizer, train_loader, test_loader = accelerator.prepare(model, optimizer, train_loader, test_loader)\n",
    "\n",
    "    accelerator.print(f'Model has {sum(p.numel() for p in model.parameters())} parameters')\n",
    "\n",
    "    # Step 3: Train the model\n",
    "    train_model(model, train_loader, optimizer, accelerator)\n",
    "\n",
    "    # Step 4: Test the model\n",
    "    test_model(model, test_loader)\n",
    "\n",
    "\n",
    "## Future Improvements\n",
    "# TODO:ajinkyak: Simple Trainer Function, custom mix of features inspired by lightning. Shift divice management to accelerate\n",
    "# TODO:ajinkyak: Flag: Overfit one batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009e0a7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## PyTorch: Our Learning Toolkit\n",
    "\n",
    "### Why PyTorch?\n",
    "ðŸ”§ **Easy to use**: Write code that looks like math  \n",
    "ðŸš€ **Powerful**: Handles the hard stuff automatically  \n",
    "ðŸ§ª **Flexible**: Great for experimentation  \n",
    "ðŸŒ **Popular**: Used by researchers and companies worldwide  \n",
    "\n",
    "### The Magic Word: `autograd`\n",
    "PyTorch automatically figures out how to learn from mistakes!\n",
    "\n",
    "#### Pytorch library\n",
    "- For writing neural networks\n",
    "- For adjusting knowledge of neural networks, to reduce error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import torch\n",
    "\n",
    "# EXAMPLE 1\n",
    "# Create a tensor with gradient tracking enabled\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define a simple function: y = x^2 + 3x + 1\n",
    "y = x**2 + 3*x + 1\n",
    "\n",
    "# Backpropagate (compute dy/dx)\n",
    "y.backward()\n",
    "\n",
    "# Gradient is stored in x.grad\n",
    "print(f\"x: {x.item()}\")\n",
    "print(f\"y: {y.item()}\")\n",
    "print(f\"dy/dx: {x.grad.item()}\")\n",
    "\n",
    "#%%\n",
    "# EXAMPLE 2\n",
    "# A vector of inputs\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# A simple function: y = sum(x^2)\n",
    "y = (x**2).sum()\n",
    "\n",
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"Gradient dy/dx:\", x.grad)\n",
    "\n",
    "\n",
    "# Why Autograd Matters\n",
    "# Neural networks: Training requires gradients of the loss w.r.t. millions of parameters â†’ Autograd handles this automatically.\n",
    "# Efficiency: Optimized C++ backend with GPU acceleration.\n",
    "# Integration: Works seamlessly with torch.optim for gradient-based optimization.\n",
    "\n",
    "\n",
    "#%%\n",
    "from torch.autograd import grad\n",
    "\n",
    "x1 = torch.tensor(2, requires_grad=True, dtype=torch.float16)\n",
    "x2 = torch.tensor(3, requires_grad=True, dtype=torch.float16)\n",
    "x3 = torch.tensor(1, requires_grad=True, dtype=torch.float16)\n",
    "x4 = torch.tensor(4, requires_grad=True, dtype=torch.float16)\n",
    "\n",
    "x1, x2, x3, x4\n",
    "\n",
    "f = x1 * x2 + x3 * x4\n",
    "\n",
    "# f = x1 * x2 + x3 * x4\n",
    "# f = 2 * 3 + 1 * 4\n",
    "# df_dx1 = 3\n",
    "# df_dx4 = 1\n",
    "\n",
    "df_dx = grad(outputs = f, inputs = [x1, x2, x3, x4])\n",
    "print(f'gradient of x1 = {df_dx[0]}')\n",
    "print(f'gradient of x2 = {df_dx[1]}')\n",
    "print(f'gradient of x3 = {df_dx[2]}')\n",
    "print(f'gradient of x4 = {df_dx[3]}')\n",
    "\n",
    "#%%\n",
    "from torch.autograd import grad\n",
    "\n",
    "x1 = torch.tensor(2, requires_grad=True, dtype=torch.float16)\n",
    "x2 = torch.tensor(3, requires_grad=True, dtype=torch.float16)\n",
    "x3 = torch.tensor(1, requires_grad=True, dtype=torch.float16)\n",
    "x4 = torch.tensor(4, requires_grad=True, dtype=torch.float16)\n",
    "\n",
    "x1, x2, x3, x4\n",
    "\n",
    "f = x1 * x2 + x3 * x4\n",
    "\n",
    "# f = x1 * x2 + x3 * x4\n",
    "# f = 2 * 3 + 1 * 4\n",
    "# df_dx1 = 3\n",
    "# df_dx4 = 1\n",
    "\n",
    "df_dx = grad(outputs = f, inputs = [x1, x2, x3, x4])\n",
    "print(f'gradient of x1 = {df_dx[0]}')\n",
    "print(f'gradient of x2 = {df_dx[1]}')\n",
    "print(f'gradient of x3 = {df_dx[2]}')\n",
    "print(f'gradient of x4 = {df_dx[3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f0104",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Training: The Learning Journey\n",
    "\n",
    "### What Happens During Training?\n",
    "\n",
    "**Epoch 1**: \"I'm just guessing randomly\" (10% accuracy)  \n",
    "**Epoch 5**: \"I'm starting to see patterns\" (50% accuracy)  \n",
    "**Epoch 10**: \"I'm getting pretty good!\" (90% accuracy)  \n",
    "**Epoch 20**: \"I'm as good as a human!\" (95%+ accuracy)  \n",
    "\n",
    "### Watching Your Model Learn\n",
    "It's like watching a child learn to read numbers!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63bd3f2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Success Metrics: How Do We Know It's Working?\n",
    "\n",
    "### The Report Card\n",
    "ðŸ“Š **Accuracy**: What percentage did we get right?  \n",
    "ðŸ“ˆ **Loss**: How \"confident\" are our wrong answers?  \n",
    "â±ï¸ **Speed**: How fast can we make decisions?  \n",
    "ðŸŽ¯ **Consistency**: Do we perform well on new data?  \n",
    "\n",
    "### Our Goal Today\n",
    "Get our model to >95% accuracy on MNIST!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a5e4c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Common Challenges & Solutions\n",
    "\n",
    "### \"My Model Isn't Learning!\"\n",
    "- Check your learning rate (not too fast, not too slow)\n",
    "- Make sure data is properly normalized\n",
    "- Verify your loss function is decreasing\n",
    "\n",
    "### \"It Works on Training but Not Test Data!\"\n",
    "- This is called \"overfitting\" - memorizing instead of learning\n",
    "- Solution: More data, simpler model, or regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce7f9eb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The Bigger Picture: Why This Matters\n",
    "\n",
    "### MNIST is Just the Beginning\n",
    "Today: Recognize handwritten digits  \n",
    "Tomorrow: Detect diseases in X-rays  \n",
    "Next week: Generate beautiful art  \n",
    "Next month: Understand natural language  \n",
    "\n",
    "### You're Learning Universal Principles\n",
    "The concepts you learn today apply to **all** of deep learning!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2681ff0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Hands-On Lab Preview\n",
    "\n",
    "### What You'll Build Today\n",
    "1. **Load MNIST data**: Get our \"textbook\" of examples\n",
    "2. **Create the network**: Build our \"brain\" \n",
    "3. **Train the model**: Let it learn from examples\n",
    "4. **Test performance**: See how well it learned\n",
    "5. **Visualize results**: See what it got right and wrong\n",
    "\n",
    "### Expected Experience\n",
    "ðŸ˜… First run: \"It's not working!\"  \n",
    "ðŸ¤” After debugging: \"Oh, I see what's wrong\"  \n",
    "ðŸ˜Š Final result: \"Wow, it really works!\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5e1f3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### What You'll Understand After Today\n",
    "ðŸŽ¯ **Core Concept**: Neural networks learn by example  \n",
    "ðŸ§  **Architecture**: Layers build up understanding  \n",
    "ðŸ“š **Training**: Iterative improvement through feedback  \n",
    "ðŸ”§ **PyTorch**: Practical tool for implementation  \n",
    "ðŸ“Š **Evaluation**: How to measure success  \n",
    "\n",
    "### Most Important Insight\n",
    "**Computers can learn to recognize patterns just like humans do - through experience and practice!**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
